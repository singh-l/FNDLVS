{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we start with the liar dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dataset\n",
    "df_train = pd.read_csv('../../../datasets/liar_tweaked/trainvectordata.csv')\n",
    "df_test = pd.read_csv('../../../datasets/liar_tweaked/testvectordata.csv')\n",
    "df_valid = pd.read_csv('../../../datasets/liar_tweaked/validvectordata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_train['statement']\n",
    "X_test=df_test['statement']\n",
    "Y_train=df_train['label']\n",
    "Y_test=df_test['label']\n",
    "X_valid=df_valid['statement']\n",
    "Y_valid=df_valid['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.append(X_valid, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=Y_test.append(Y_valid, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.str.replace('[^a-zA-Z]',' ').str.lower()\n",
    "X_train = X_train.str.replace('[^a-zA-Z]',' ').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    building  wall   u   mexico border  take liter...\n",
       "1     wisconsin   pace  double  number  layoffs  year \n",
       "2          says john mccain  done nothing  help  vets \n",
       "3    suzanne bonamici supports  plan   cut choice  ...\n",
       "4     asked   reporter whether hes   center   crimi...\n",
       "Name: statement, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words with regex. '\\\\b' matches any break (space or linebreak or whatever) and '|'\n",
    "# is an or operator. So, for example '\\\\ba\\\\b|\\\\bis\\\\b|\\\\band\\\\b' will match 'a', 'is' or 'and'.\n",
    "stop_re = '\\\\b'+'\\\\b|\\\\b'.join(nltk.corpus.stopwords.words('english'))+'\\\\b'\n",
    "X_test = X_test.str.replace(stop_re, '')\n",
    "X_train = X_train.str.replace(stop_re, '')\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for i in range(0, len(X_test)):\n",
    "    s=X_test[i]\n",
    "    st=\"\"\n",
    "    st+=\" \".join([ps.stem(i) for i in s.split()])\n",
    "    X_test[i]=st\n",
    "for i in range(0, len(X_train)):\n",
    "    s=X_train[i]\n",
    "    st=\"\"\n",
    "    st+=\" \".join([ps.stem(i) for i in s.split()])\n",
    "    X_train[i]=st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "X_test = X_test.str.split()\n",
    "X_train = X_train.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vtest = gensim.models.Word2Vec(X_test.tolist(), min_count=1)\n",
    "w2vtrain = gensim.models.Word2Vec(X_train.tolist(), min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data,w2v):\n",
    "    for i in range(0, len(data)):\n",
    "        x=data[i]\n",
    "        b = np.zeros(100) \n",
    "        for w in x:\n",
    "            b+=w2v[w]\n",
    "        b = np.divide(b, 100)\n",
    "        x=b\n",
    "        data[i]=x\n",
    "        #print(b)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lovedeepsingh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_test=transform_data(X_test,w2vtest)\n",
    "X_train=transform_data(X_train,w2vtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.tolist())\n",
    "X_test = np.array(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train.tolist())\n",
    "Y_test = np.array(Y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 161,001\n",
      "Trainable params: 161,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "## Creating model\n",
    "\n",
    "# ## expand train dimnesion: pass from 2d to 3d\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "model=Sequential()\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False), input_shape=(1, 100)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10229 samples, validate on 2551 samples\n",
      "Epoch 1/20\n",
      "10229/10229 [==============================] - 12s 1ms/sample - loss: 0.6852 - accuracy: 0.5603 - val_loss: 0.6898 - val_accuracy: 0.5417\n",
      "Epoch 2/20\n",
      "10229/10229 [==============================] - 3s 308us/sample - loss: 0.6832 - accuracy: 0.5619 - val_loss: 0.6903 - val_accuracy: 0.5417\n",
      "Epoch 3/20\n",
      "10229/10229 [==============================] - 3s 302us/sample - loss: 0.6819 - accuracy: 0.5622 - val_loss: 0.6905 - val_accuracy: 0.5417\n",
      "Epoch 4/20\n",
      "10229/10229 [==============================] - 3s 332us/sample - loss: 0.6807 - accuracy: 0.5658 - val_loss: 0.6909 - val_accuracy: 0.5417\n",
      "Epoch 5/20\n",
      "10229/10229 [==============================] - 3s 324us/sample - loss: 0.6785 - accuracy: 0.5676 - val_loss: 0.6921 - val_accuracy: 0.5414\n",
      "Epoch 6/20\n",
      "10229/10229 [==============================] - 3s 320us/sample - loss: 0.6762 - accuracy: 0.5727 - val_loss: 0.6932 - val_accuracy: 0.5382\n",
      "Epoch 7/20\n",
      "10229/10229 [==============================] - 3s 340us/sample - loss: 0.6742 - accuracy: 0.5781 - val_loss: 0.6927 - val_accuracy: 0.5437\n",
      "Epoch 8/20\n",
      "10229/10229 [==============================] - 3s 335us/sample - loss: 0.6742 - accuracy: 0.5759 - val_loss: 0.6917 - val_accuracy: 0.5410\n",
      "Epoch 9/20\n",
      "10229/10229 [==============================] - 3s 342us/sample - loss: 0.6736 - accuracy: 0.5778 - val_loss: 0.6917 - val_accuracy: 0.5410\n",
      "Epoch 10/20\n",
      "10229/10229 [==============================] - 3s 329us/sample - loss: 0.6736 - accuracy: 0.5781 - val_loss: 0.6986 - val_accuracy: 0.4583\n",
      "Epoch 11/20\n",
      "10229/10229 [==============================] - 4s 357us/sample - loss: 0.6725 - accuracy: 0.5806 - val_loss: 0.6923 - val_accuracy: 0.5410\n",
      "Epoch 12/20\n",
      "10229/10229 [==============================] - 3s 338us/sample - loss: 0.6719 - accuracy: 0.5796 - val_loss: 0.6918 - val_accuracy: 0.5410\n",
      "Epoch 13/20\n",
      "10229/10229 [==============================] - 3s 340us/sample - loss: 0.6723 - accuracy: 0.5833 - val_loss: 0.6963 - val_accuracy: 0.4583\n",
      "Epoch 14/20\n",
      "10229/10229 [==============================] - 3s 335us/sample - loss: 0.6725 - accuracy: 0.5785 - val_loss: 0.6961 - val_accuracy: 0.4583\n",
      "Epoch 15/20\n",
      "10229/10229 [==============================] - 4s 372us/sample - loss: 0.6727 - accuracy: 0.5763 - val_loss: 0.6932 - val_accuracy: 0.5335\n",
      "Epoch 16/20\n",
      "10229/10229 [==============================] - 4s 421us/sample - loss: 0.6723 - accuracy: 0.5826 - val_loss: 0.6950 - val_accuracy: 0.4583\n",
      "Epoch 17/20\n",
      "10229/10229 [==============================] - 3s 329us/sample - loss: 0.6712 - accuracy: 0.5817 - val_loss: 0.6971 - val_accuracy: 0.4583\n",
      "Epoch 18/20\n",
      "10229/10229 [==============================] - 4s 418us/sample - loss: 0.6717 - accuracy: 0.5833 - val_loss: 0.6972 - val_accuracy: 0.4583\n",
      "Epoch 19/20\n",
      "10229/10229 [==============================] - 4s 412us/sample - loss: 0.6714 - accuracy: 0.5795 - val_loss: 0.6982 - val_accuracy: 0.4583\n",
      "Epoch 20/20\n",
      "10229/10229 [==============================] - 3s 337us/sample - loss: 0.6717 - accuracy: 0.5805 - val_loss: 0.6985 - val_accuracy: 0.4583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a442cc320>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=20,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we have the kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../datasets/kaggle/train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['title'] + ' ' + df['text']\n",
    "df=df[df['content']==df['content']]\n",
    "df=df[df['label']==df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../datasets/kaggle/final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../../datasets/kaggle/final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['content']\n",
    "Y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.str.replace('[^a-zA-Z]',' ').str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    house dem aide     even see comey  letter  jas...\n",
       "1    flynn  hillary clinton  big woman  campus   br...\n",
       "2      truth might get  fired   truth might get  fi...\n",
       "3       civilians killed  single us airstrike   ide...\n",
       "4    iranian woman jailed  fictional unpublished st...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_re = '\\\\b'+'\\\\b|\\\\b'.join(nltk.corpus.stopwords.words('english'))+'\\\\b'\n",
    "X = X.str.replace(stop_re, '')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for i in range(0, len(X)):\n",
    "    s=X[i]\n",
    "    st=\"\"\n",
    "    st+=\" \".join([ps.stem(i) for i in s.split()])\n",
    "    X[i]=st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "X = X.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vtest = gensim.models.Word2Vec(X_test.tolist(), min_count=1)\n",
    "w2vtrain = gensim.models.Word2Vec(X_train.tolist(), min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.reset_index()\n",
    "Y_test=Y_test.reset_index()\n",
    "X_train=X_train.reset_index()\n",
    "Y_train=Y_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=Y_test['label']\n",
    "X_test=X_test['content']\n",
    "X_train=X_train['content']\n",
    "Y_train=Y_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lovedeepsingh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_test=transform_data(X_test,w2vtest)\n",
    "X_train=transform_data(X_train,w2vtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.tolist())\n",
    "X_test = np.array(X_test.tolist())\n",
    "Y_train = np.array(Y_train.tolist())\n",
    "Y_test = np.array(Y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 161,001\n",
      "Trainable params: 161,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "## Creating model\n",
    "\n",
    "# ## expand train dimnesion: pass from 2d to 3d\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "model=Sequential()\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False), input_shape=(1, 100)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15152 samples, validate on 5051 samples\n",
      "Epoch 1/20\n",
      "15152/15152 [==============================] - 13s 862us/sample - loss: 0.3388 - accuracy: 0.8652 - val_loss: 0.5754 - val_accuracy: 0.7387\n",
      "Epoch 2/20\n",
      "15152/15152 [==============================] - 5s 306us/sample - loss: 0.2527 - accuracy: 0.9025 - val_loss: 0.7448 - val_accuracy: 0.7123\n",
      "Epoch 3/20\n",
      "15152/15152 [==============================] - 5s 350us/sample - loss: 0.2285 - accuracy: 0.9073 - val_loss: 0.8407 - val_accuracy: 0.6989\n",
      "Epoch 4/20\n",
      "15152/15152 [==============================] - 5s 335us/sample - loss: 0.2135 - accuracy: 0.9152 - val_loss: 0.9521 - val_accuracy: 0.6892\n",
      "Epoch 5/20\n",
      "15152/15152 [==============================] - 6s 364us/sample - loss: 0.2043 - accuracy: 0.9171 - val_loss: 0.7854 - val_accuracy: 0.7090\n",
      "Epoch 6/20\n",
      "15152/15152 [==============================] - 5s 308us/sample - loss: 0.1952 - accuracy: 0.9211 - val_loss: 0.9049 - val_accuracy: 0.6923\n",
      "Epoch 7/20\n",
      "15152/15152 [==============================] - 5s 316us/sample - loss: 0.1871 - accuracy: 0.9236 - val_loss: 1.0576 - val_accuracy: 0.6836\n",
      "Epoch 8/20\n",
      "15152/15152 [==============================] - 5s 305us/sample - loss: 0.1851 - accuracy: 0.9228 - val_loss: 1.0008 - val_accuracy: 0.6844\n",
      "Epoch 9/20\n",
      "15152/15152 [==============================] - 5s 311us/sample - loss: 0.1766 - accuracy: 0.9304 - val_loss: 0.9458 - val_accuracy: 0.6917\n",
      "Epoch 10/20\n",
      "15152/15152 [==============================] - 5s 298us/sample - loss: 0.1757 - accuracy: 0.9297 - val_loss: 0.9404 - val_accuracy: 0.6838\n",
      "Epoch 11/20\n",
      "15152/15152 [==============================] - 4s 292us/sample - loss: 0.1684 - accuracy: 0.9332 - val_loss: 0.9250 - val_accuracy: 0.6904\n",
      "Epoch 12/20\n",
      "15152/15152 [==============================] - 4s 294us/sample - loss: 0.1643 - accuracy: 0.9349 - val_loss: 0.9941 - val_accuracy: 0.6832\n",
      "Epoch 13/20\n",
      "15152/15152 [==============================] - 4s 293us/sample - loss: 0.1601 - accuracy: 0.9360 - val_loss: 1.0496 - val_accuracy: 0.6803\n",
      "Epoch 14/20\n",
      "15152/15152 [==============================] - 4s 284us/sample - loss: 0.1591 - accuracy: 0.9368 - val_loss: 0.9855 - val_accuracy: 0.6898\n",
      "Epoch 15/20\n",
      "15152/15152 [==============================] - 4s 295us/sample - loss: 0.1547 - accuracy: 0.9383 - val_loss: 0.9449 - val_accuracy: 0.6961\n",
      "Epoch 16/20\n",
      "15152/15152 [==============================] - 6s 402us/sample - loss: 0.1503 - accuracy: 0.9413 - val_loss: 0.9069 - val_accuracy: 0.7014\n",
      "Epoch 17/20\n",
      "15152/15152 [==============================] - 10s 646us/sample - loss: 0.1496 - accuracy: 0.9411 - val_loss: 0.9796 - val_accuracy: 0.6921\n",
      "Epoch 18/20\n",
      "15152/15152 [==============================] - 8s 545us/sample - loss: 0.1458 - accuracy: 0.9430 - val_loss: 0.9733 - val_accuracy: 0.6949\n",
      "Epoch 19/20\n",
      "15152/15152 [==============================] - 6s 382us/sample - loss: 0.1430 - accuracy: 0.9442 - val_loss: 0.9934 - val_accuracy: 0.6955\n",
      "Epoch 20/20\n",
      "15152/15152 [==============================] - 9s 569us/sample - loss: 0.1393 - accuracy: 0.9446 - val_loss: 1.0363 - val_accuracy: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a44384f60>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=20,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
